"""
Chainlit Web UI for Self-RAG Turkish Educational Q&A System
Interactive interface for MEB document queries using Self-RAG
"""
import chainlit as cl
import asyncio
import time
import logging
from typing import Dict, Any, Optional

from main import InteractiveSelfRAG

# Configure logging for web UI
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global pipeline instance
pipeline: Optional[InteractiveSelfRAG] = None

@cl.on_chat_start
async def start():
    """Initialize the Self-RAG pipeline when chat starts"""
    global pipeline
    
    # Show startup message
    await cl.Message(
        content="ğŸš€ **Self-RAG TÃ¼rkÃ§e EÄŸitim Sistemi** baÅŸlatÄ±lÄ±yor...\n\n"
               "MEB dokÃ¼manlarÄ± Ã¼zerinde Self-RAG teknolojisi ile soru-cevap sistemi.\n\n"
               "â³ LÃ¼tfen sistemin yÃ¼klenmesini bekleyin...",
        author="Sistem"
    ).send()
    
    try:
        # Initialize pipeline
        pipeline = InteractiveSelfRAG()
        
        # Update user about progress
        await cl.Message(
            content="ğŸ“„ PDF dosyalarÄ± iÅŸleniyor...",
            author="Sistem"
        ).send()
        
        # Run the full pipeline
        success = await asyncio.to_thread(pipeline.setup_pipeline)
        
        if success:
            # Show success message with system info
            stats = get_system_stats()
            await cl.Message(
                content=f"âœ… **Sistem hazÄ±r!** \n\n"
                       f"ğŸ“Š **Sistem Bilgileri:**\n"
                       f"â€¢ ğŸ“„ Ä°ÅŸlenen PDF: {stats['pdf_count']} dosya\n"
                       f"â€¢ âœ‚ï¸ OluÅŸturulan chunk: {stats['chunk_count']}\n"
                       f"â€¢ ğŸ”¤ Embedding boyutu: {stats['embedding_dim']}\n"
                       f"â€¢ ğŸ§  LLM modeli: {stats['llm_model']}\n"
                       f"â€¢ ğŸ—„ï¸ VektÃ¶r DB: Vespa\n\n"
                       f"ğŸ¯ **ArtÄ±k soru sorabilirsiniz!**\n\n"
                       f"*Ã–rnek sorular:*\n"
                       f"â€¢ TÃ¼rkiye'de eÄŸitim sistemi nasÄ±l dÃ¼zenlenmiÅŸtir?\n"
                       f"â€¢ Ã–ÄŸretmen yeterlilikleri nasÄ±l belirlenir?\n"
                       f"â€¢ MÃ¼fredat geliÅŸtirme sÃ¼reÃ§leri kimler tarafÄ±ndan yÃ¼rÃ¼tÃ¼lÃ¼r?",
                author="Sistem"
            ).send()
            
            # Set user session info
            cl.user_session.set("pipeline", pipeline)
            cl.user_session.set("ready", True)
        else:
            await cl.Message(
                content="âŒ Sistem baÅŸlatÄ±lamadÄ±. LÃ¼tfen logs'u kontrol edin.",
                author="Sistem"
            ).send()
            cl.user_session.set("ready", False)
            
    except Exception as e:
        logger.error(f"Error starting pipeline: {str(e)}")
        await cl.Message(
            content=f"âŒ **Hata:** {str(e)}\n\n"
                   f"LÃ¼tfen ÅŸunlarÄ± kontrol edin:\n"
                   f"â€¢ Ollama servisi Ã§alÄ±ÅŸÄ±yor mu? (`ollama serve`)\n"
                   f"â€¢ Docker servisi aktif mi?\n"
                   f"â€¢ PDF dosyalarÄ± mevcut mu?",
            author="Sistem"
        ).send()
        cl.user_session.set("ready", False)

@cl.on_message
async def main(message: cl.Message):
    """Handle user questions"""
    pipeline = cl.user_session.get("pipeline")
    ready = cl.user_session.get("ready", False)
    
    if not ready or not pipeline:
        await cl.Message(
            content="âš ï¸ Sistem henÃ¼z hazÄ±r deÄŸil. LÃ¼tfen baÅŸlatÄ±lmasÄ±nÄ± bekleyin.",
            author="Sistem"
        ).send()
        return
    
    user_question = message.content.strip()
    
    if not user_question:
        await cl.Message(
            content="â“ LÃ¼tfen bir soru yazÄ±n.",
            author="Sistem"
        ).send()
        return
    
    # Show processing message
    processing_msg = await cl.Message(
        content=f"ğŸ¤” **Sorunuz iÅŸleniyor:** {user_question}\n\n"
               f"â³ Self-RAG grafu Ã§alÄ±ÅŸÄ±yor...\n"
               f"ğŸ” AlakalÄ± belgeler aranÄ±yor...\n"
               f"ğŸ§  LLM cevap Ã¼retiyor...",
        author="Self-RAG"
    ).send()
    
    try:
        start_time = time.time()
        
        # Process question through Self-RAG
        result = await asyncio.to_thread(pipeline.ask_question, user_question)
        
        processing_time = time.time() - start_time
        
        # Format response
        response_content = format_response(result, processing_time)
        
        # Send final result as new message
        await cl.Message(
            content=response_content,
            author="Self-RAG"
        ).send()
        
        # Log the interaction
        logger.info(f"Question answered: {user_question} -> {len(result['answer'])} chars")
        
    except Exception as e:
        logger.error(f"Error processing question: {str(e)}")
        await cl.Message(
            content=f"âŒ **Hata oluÅŸtu:** {str(e)}\n\n"
                   f"LÃ¼tfen sorunuzu yeniden deneyin.",
            author="Sistem"
        ).send()

def get_system_stats() -> Dict[str, Any]:
    """Get system statistics for display"""
    global pipeline
    
    if not pipeline:
        return {}
    
    try:
        # Get basic stats
        pdf_stats = pipeline.pdf_processor.get_total_content_stats()
        chunk_stats = pipeline.chunker.get_chunking_stats(pipeline.chunks)
        embedding_dim = pipeline.embeddings.get_embedding_dimension()
        
        return {
            'pdf_count': pdf_stats['total_files'],
            'chunk_count': chunk_stats['total_chunks'],
            'embedding_dim': embedding_dim,
            'llm_model': 'llama3.2:1b'
        }
    except:
        return {
            'pdf_count': '?',
            'chunk_count': '?', 
            'embedding_dim': '?',
            'llm_model': 'llama3.2:1b'
        }

def format_response(result: Dict[str, Any], processing_time: float) -> str:
    """Format the Self-RAG response for display"""
    
    answer = result.get('answer', 'Cevap Ã¼retilemedi.')
    sources = result.get('sources', [])
    retrieval_attempts = result.get('retrieval_attempts', 0)
    rewritten_question = result.get('rewritten_question', '')
    
    # Build response
    response = f"ğŸ“ **Cevap:**\n{answer}\n\n"
    
    # Add sources if available
    if sources:
        source_list = '\n'.join([f"â€¢ {source}" for source in sources])
        response += f"ğŸ“š **Kaynaklar:**\n{source_list}\n\n"
    else:
        response += "ğŸ“š **Kaynaklar:** AlakalÄ± belge bulunamadÄ±\n\n"
    
    # Add processing details
    response += f"ğŸ“Š **Ä°ÅŸlem DetaylarÄ±:**\n"
    response += f"â€¢ â±ï¸ SÃ¼re: {processing_time:.1f} saniye\n"
    response += f"â€¢ ğŸ”„ Arama sayÄ±sÄ±: {retrieval_attempts}\n"
    
    if rewritten_question and rewritten_question != result.get('question', ''):
        response += f"â€¢ âœï¸ Soru yeniden yazÄ±ldÄ±: *{rewritten_question}*\n"
    
    # Add Self-RAG workflow info
    response += f"\nğŸ”„ **Self-RAG SÃ¼reci:**\n"
    if retrieval_attempts > 1:
        response += f"â€¢ isREL: Belge alakalÄ±lÄ±ÄŸÄ± kontrol edildi\n"
        response += f"â€¢ Query rewriting: Soru optimize edildi\n"
    response += f"â€¢ isSUP: HalÃ¼sinasyon kontrolÃ¼ yapÄ±ldÄ±\n"
    response += f"â€¢ Cevap kalitesi deÄŸerlendirildi\n"
    
    return response

@cl.on_chat_end
async def end():
    """Cleanup when chat ends"""
    global pipeline
    
    if pipeline:
        try:
            await asyncio.to_thread(pipeline.cleanup)
            logger.info("Pipeline cleaned up")
        except Exception as e:
            logger.error(f"Error during cleanup: {str(e)}")
        
        pipeline = None

if __name__ == "__main__":
    import os
    
    # Set environment for better display
    os.environ["CHAINLIT_AUTH_SECRET"] = "self-rag-secret"
    
    # Start the Chainlit app
    print("ğŸš€ Self-RAG Chainlit UI baÅŸlatÄ±lÄ±yor...")
    print("ğŸ“± Web arayÃ¼zÃ¼: http://localhost:8000")
    print("â¹ï¸  Durdurmak iÃ§in: Ctrl+C") 